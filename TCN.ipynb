{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, ConfusionMatrixDisplay\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input, Flatten, Embedding, Concatenate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tcn import TCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature engineering\n",
    "\n",
    "demand_weather_merged = pd.read_parquet('data/demand_weather_merged_0.5.parquet')\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "features = demand_weather_merged[['temperature_2m', 'precipitation', 'month', \n",
    "                                   'snow_depth', 'pressure_msl', 'cloud_cover', \n",
    "                                   'sunshine_duration', \n",
    "                                   'day_of_week', 'building_id_encoded']]\n",
    "target = demand_weather_merged['y']\n",
    "\n",
    "target_log_transformed = np.log1p(target)\n",
    "target_clipped = np.clip(target_log_transformed, target_log_transformed.quantile(0.05), target_log_transformed.quantile(0.95))\n",
    "\n",
    "features_df = pd.DataFrame(features)\n",
    "constant_features = features_df.columns[features_df.nunique() <= 1]\n",
    "features_df = features_df.drop(columns=constant_features)\n",
    "\n",
    "\n",
    "# Input sequence for forecasting in the model window_size = weeks\n",
    "window_size = 4\n",
    "\n",
    "def create_sequences(features, target, window_size):\n",
    "    X, y = [], []\n",
    "    for i in range(len(features) - window_size):\n",
    "        X.append(features[i:i + window_size])\n",
    "        y.append(target[i + window_size])\n",
    "    return np.array(X, dtype='float32'), np.array(y, dtype='float32')\n",
    "\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features_df)\n",
    "\n",
    "X, y = create_sequences(features_scaled, target_log_transformed.values, window_size)\n",
    "building_ids = features_df['building_id_encoded'].values[window_size:]\n",
    "\n",
    "# Changing continuous variable in dataset to discrete for classification\n",
    "bin_edges = [0, 28, 56, 84, np.inf]\n",
    "num_classes = len(bin_edges) - 1\n",
    "\n",
    "demand_weather_merged['y_bin'] = np.digitize(demand_weather_merged['y'], bins=bin_edges, right=False) - 1\n",
    "y_bin_sequenced = demand_weather_merged['y_bin'].values[window_size:]\n",
    "\n",
    "# 80% training and 20% temp\n",
    "X_train, X_temp, y_train, y_temp, train_building_ids, temp_building_ids = train_test_split(\n",
    "    X, y_bin_sequenced, building_ids, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Split the 20% temp set into 50% validation and 50% test\n",
    "X_val, X_test, y_val, y_test, val_building_ids, test_building_ids = train_test_split(\n",
    "    X_temp, y_temp, temp_building_ids, test_size=0.5, random_state=42\n",
    ")\n",
    "\n",
    "y_train_categorical = to_categorical(y_train, num_classes=num_classes)\n",
    "y_val_categorical = to_categorical(y_val, num_classes=num_classes)\n",
    "y_test_categorical = to_categorical(y_test, num_classes=num_classes)\n",
    "\n",
    "y_train_categorical = to_categorical(y_train, num_classes=num_classes)\n",
    "y_test_categorical = to_categorical(y_test, num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model and training\n",
    "\n",
    "def build_tcn(input_shape, num_buildings, learning_rate=0.001):\n",
    "    time_series_input = Input(shape=input_shape, name=\"time_series_input\")\n",
    "    building_input = Input(shape=(1,), name=\"building_id\")\n",
    "\n",
    "    tcn_output = TCN(kernel_size=3,\n",
    "                     nb_filters=32,\n",
    "                     dilations=[1, 2, 4, 8],\n",
    "                     activation='relu',\n",
    "                     dropout_rate=0.1,\n",
    "                     return_sequences=False)(time_series_input)\n",
    "\n",
    "    embedding_size = 64\n",
    "    building_embedding = Embedding(input_dim=num_buildings + 1, output_dim=embedding_size)(building_input)\n",
    "    building_embedding = Flatten()(building_embedding)\n",
    "\n",
    "    combined = Concatenate()([tcn_output, building_embedding])\n",
    "\n",
    "    dense = Dense(32, activation='relu')(combined)\n",
    "    dropout = Dropout(0.3)(dense)\n",
    "    outputs = Dense(num_classes, activation='softmax')(dropout)\n",
    "\n",
    "    model = Model(inputs=[time_series_input, building_input], outputs=outputs)\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "input_shape = (window_size, X_train.shape[2])\n",
    "learning_rate = 0.001\n",
    "num_buildings = demand_weather_merged['building_id_encoded'].nunique()\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "tcn_model = build_tcn(input_shape, num_buildings, learning_rate)\n",
    "history = tcn_model.fit(\n",
    "    [X_train, train_building_ids],\n",
    "    y_train_categorical,\n",
    "    validation_data=([X_val, val_building_ids], y_val_categorical),\n",
    "    epochs=25, \n",
    "    batch_size=32,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "val_loss, val_accuracy = tcn_model.evaluate([X_val, val_building_ids], y_val_categorical)\n",
    "print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "y_pred = tcn_model.predict([X_test, test_building_ids])\n",
    "\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_test_classes = np.argmax(y_test_categorical, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving models\n",
    "\n",
    "tcn_model.save('saved_model/tcn_model_0.5_classified.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Use an old model\n",
    "\n",
    "# from tensorflow.keras.models import load_model\n",
    "\n",
    "# model = load_model('saved_model/tcn_model_0.5_classified.h5', custom_objects={'TCN': TCN})\n",
    "\n",
    "# demand_weather_merged = pd.read_parquet('data/demand_weather_merged_0.5.parquet')\n",
    "\n",
    "# label_encoder = LabelEncoder()\n",
    "\n",
    "# features = demand_weather_merged[['temperature_2m', 'precipitation', 'month', \n",
    "#                                    'snow_depth', 'pressure_msl', 'cloud_cover', \n",
    "#                                    'sunshine_duration', \n",
    "#                                    'day_of_week', 'building_id_encoded']]\n",
    "# target = demand_weather_merged['y']\n",
    "\n",
    "# target_log_transformed = np.log1p(target)\n",
    "# target_clipped = np.clip(target_log_transformed, target_log_transformed.quantile(0.05), target_log_transformed.quantile(0.95))\n",
    "\n",
    "# label_encoder = LabelEncoder()\n",
    "\n",
    "# features_df = pd.DataFrame(features)\n",
    "# constant_features = features_df.columns[features_df.nunique() <= 1]\n",
    "# features_df = features_df.drop(columns=constant_features)\n",
    "\n",
    "# window_size = 4\n",
    "\n",
    "# def create_sequences(features, target, window_size):\n",
    "#     X, y = [], []\n",
    "#     for i in range(len(features) - window_size):\n",
    "#         X.append(features[i:i + window_size])\n",
    "#         y.append(target[i + window_size])\n",
    "#     return np.array(X, dtype='float32'), np.array(y, dtype='float32')\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# features_scaled = scaler.fit_transform(features_df)\n",
    "\n",
    "# X, y = create_sequences(features_scaled, target_log_transformed.values, window_size)\n",
    "# building_ids = features_df['building_id_encoded'].values[window_size:]\n",
    "\n",
    "# bin_edges = [0, 28, 56, 84, np.inf]\n",
    "# num_classes = len(bin_edges) - 1\n",
    "\n",
    "# demand_weather_merged['y_bin'] = np.digitize(demand_weather_merged['y'], bins=bin_edges, right=False) - 1\n",
    "# y_bin_sequenced = demand_weather_merged['y_bin'].values[window_size:]\n",
    "\n",
    "# X_train, X_test, y_train, y_test, train_building_ids, test_building_ids = train_test_split(\n",
    "#     X, y_bin_sequenced, building_ids, test_size=0.2, random_state=42\n",
    "# )\n",
    "\n",
    "# y_train_categorical = to_categorical(y_train, num_classes=num_classes)\n",
    "# y_test_categorical = to_categorical(y_test, num_classes=num_classes)\n",
    "\n",
    "# y_pred = model.predict([X_test, test_building_ids])\n",
    "\n",
    "# y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "# y_test_classes = np.argmax(y_test_categorical, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metric analysis\n",
    "\n",
    "accuracy = accuracy_score(y_test_classes, y_pred_classes)\n",
    "precision = precision_score(y_test_classes, y_pred_classes, average='weighted')\n",
    "recall = recall_score(y_test_classes, y_pred_classes, average='weighted')\n",
    "f1 = f1_score(y_test_classes, y_pred_classes, average='weighted')\n",
    "\n",
    "print(f\"\\nClassification Metrics:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_classes, y_pred_classes))\n",
    "\n",
    "ConfusionMatrixDisplay.from_predictions(y_test_classes, y_pred_classes, normalize='true')\n",
    "plt.show()\n",
    "\n",
    "# # Both plots below can only be seen after training not when model is loaded\n",
    "# # accuracy vs epochs\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.plot(history.history['accuracy'], label='Training Accuracy', color='blue')\n",
    "# plt.plot(history.history['val_accuracy'], label='Validation Accuracy', color='red')\n",
    "# plt.title('Accuracy vs. Epochs')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.show()\n",
    "\n",
    "# # loss vs epochs\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.plot(history.history['loss'], label='Training Loss', color='blue')\n",
    "# plt.plot(history.history['val_loss'], label='Validation Loss', color='red')\n",
    "# plt.title('Loss vs. Epochs')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
