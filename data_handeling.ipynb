{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__DATA HANDELING__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File is only used to create edited datasets. Allows for testing different sizes or resampling techniques, to run this the original files from hugging face need to be downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data\n",
    "\n",
    "# Need to download the original files from huggingface\n",
    "demand_data = pd.read_parquet('data/demand.parquet')\n",
    "metadata = pd.read_parquet('data/metadata.parquet')\n",
    "weather_data = pd.read_parquet('data/weather.parquet')\n",
    " \n",
    "demand_data['timestamp'] = pd.to_datetime(demand_data['timestamp'])\n",
    "metadata['location_id'] = metadata['location_id'].astype(str)\n",
    "weather_data['timestamp'] = pd.to_datetime(weather_data['timestamp'])\n",
    "weather_data['location_id'] = weather_data['location_id'].astype(str)\n",
    "\n",
    "# Sampling, increase if have more compute power or get rid of\n",
    "demand_data_sampled = demand_data.sample(frac=0.8, random_state=42)\n",
    "demand_weather_merged = pd.merge(demand_data_sampled, metadata, on='unique_id', how='inner')\n",
    "demand_weather_merged = pd.merge(demand_weather_merged, weather_data, on=['timestamp', 'location_id'], how='left')\n",
    "\n",
    "# Only want to see London Smart Meter\n",
    "demand_weather_merged = demand_weather_merged[demand_weather_merged['dataset'] == 'London Smart Meter Data']\n",
    "\n",
    "# Dropping empty or unused columns\n",
    "demand_weather_merged.ffill(inplace=True)\n",
    "demand_weather_merged.dropna(inplace=True)\n",
    "demand_weather_merged.drop(['unique_id', 'location_id'], axis=1, inplace=True)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "demand_weather_merged['building_id_encoded'] = label_encoder.fit_transform(demand_weather_merged['building_id'])\n",
    "\n",
    "demand_weather_merged['hour'] = demand_weather_merged['timestamp'].dt.hour\n",
    "demand_weather_merged['day_of_week'] = demand_weather_merged['timestamp'].dt.dayofweek\n",
    "demand_weather_merged['month'] = demand_weather_merged['timestamp'].dt.month\n",
    "\n",
    "# Resampling to be able to load more data\n",
    "def resample_building_data(group):\n",
    "    group.set_index('timestamp', inplace=True)\n",
    "    resampled = group.resample('W').agg({\n",
    "        'temperature_2m': 'mean', \n",
    "        'precipitation': 'sum',\n",
    "        'snow_depth': 'mean',\n",
    "        'pressure_msl': 'mean',\n",
    "        'cloud_cover': 'mean',\n",
    "        'sunshine_duration': 'mean',\n",
    "        'y': 'sum',\n",
    "        'day_of_week': 'mean',\n",
    "        'month': 'first',\n",
    "        'building_id_encoded': 'first'\n",
    "    })\n",
    "    resampled.reset_index(inplace=True)\n",
    "    return resampled\n",
    "\n",
    "demand_weather_merged = demand_weather_merged.groupby('building_id').apply(resample_building_data)\n",
    "demand_weather_merged.reset_index(drop=True, inplace=True)\n",
    "\n",
    "demand_weather_merged.sort_values(by='timestamp', inplace=True)\n",
    "\n",
    "# Save parquet file\n",
    "demand_weather_merged.to_parquet('data/demand_weather_merged_0.8.parquet', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
